import sys
from datetime import datetime

import gflags
import tensorflow as tf
from tensorflow.contrib import rnn

from model.tracking_by_natural_language_description.tensorflow_vgg.vgg16 import Vgg16

gflags.DEFINE_string('restore_inld_file', '', 'File path to restore INLD model. Leave empty to train from scratch')
gflags.DEFINE_string('training_file', '/scratch2/data/coco_caption_embedding.tfrecords',
                     'File path to the training file')
gflags.DEFINE_integer('max_iter', 20000, 'Max Iteration to Train the Model')
gflags.DEFINE_string('inld_training_file', '/scratch2/data/coco_caption_embedding.tfrecords',
                     'Training file as generated by coco_caption_to_embedding_tfrecords.py')
gflags.DEFINE_string('inld_validation_file', '/scratch2/data/coco_caption_validation.tfrecords',
                     'Validation file as generated by coco_caption_to_embedding_tfrecords.py')
gflags.DEFINE_float('inld_keep_prob', 0.9, 'Keep rate for dropouts.')
gflags.DEFINE_integer('batch_size', 64,
                      'Batch size for INLD, default is 64 for training. Use 1 for excitation back propagation')
gflags.DEFINE_boolean('excitation_back_propagation', False, 'Whether or not to run the excitation back propagation.')

FLAGS = gflags.FLAGS


class ImageNaturalLanguageDetector:
    def __init__(self, filename):
        self.batch_size = FLAGS.batch_size
        self.image_shape = [224, 224, 3]
        self.sample_size = 2
        self.num_threads = 1

        self.inld_graph = tf.Graph()
        with self.inld_graph.as_default():
            self.global_step = tf.Variable(1, name='global_step', trainable=False)
            with tf.name_scope('input'):
                self.training_data_files = [filename]

                filename_queue = tf.train.string_input_producer(self.training_data_files)
                reader = tf.TFRecordReader()
                _, serialized_example = reader.read(filename_queue)
                example = tf.parse_single_example(
                    serialized_example,
                    features={
                        'images': tf.FixedLenFeature([self.sample_size], tf.string),
                        'gt_index': tf.FixedLenFeature([], tf.int64),
                        'gt_similarity': tf.FixedLenFeature([], tf.string),
                        'embedding_caption': tf.FixedLenFeature([], tf.string),
                        'caption': tf.FixedLenFeature([], tf.string),
                    })

                images_raw = example['images']
                gt_index = example['gt_index']
                gt_similarity = example['gt_similarity']
                query = example['embedding_caption']
                caption = example['caption']

                [self.images_raw_batch, self.gt_index_batch, self.gt_similarity_batch, self.query_batch,
                 self.caption_batch] = tf.train.shuffle_batch(
                    [images_raw, gt_index, gt_similarity, query, caption],
                    batch_size=self.batch_size,
                    capacity=self.batch_size * 2 + 1,
                    num_threads=self.num_threads,
                    min_after_dequeue=self.batch_size)

                self.images = tf.cast(tf.decode_raw(tf.unstack(self.images_raw_batch), tf.uint8), tf.float32)
                self.gt_similarity_batch = tf.reshape(
                    tf.decode_raw(tf.unstack(self.gt_similarity_batch, axis=0), tf.float32),
                    shape=[self.batch_size, self.sample_size])
                self.q = []
                for query in tf.unstack(self.query_batch, axis=0):
                    self.q.append(tf.reshape(tf.decode_raw(query, tf.float32), shape=[-1, 300]))

            with tf.name_scope('vgg16'):
                tf.logging.debug('Creating VGG16 representation layer')
                images = tf.reshape(self.images, shape=[self.batch_size * self.sample_size] + self.image_shape)
                self.vgg = Vgg16(images)
                convolution_features = self.vgg.fc3l
                if not FLAGS.excitation_back_propagation:
                    convolution_features = tf.nn.dropout(convolution_features, keep_prob=FLAGS.inld_keep_prob)
                self.fully_connected_weight = tf.Variable(tf.truncated_normal([1000, 300],
                                                                              dtype=tf.float32,
                                                                              stddev=1e-1), name='weights')
                self.fully_connected_bias = tf.Variable(tf.constant(1.0, shape=[300], dtype=tf.float32),
                                                        trainable=True, name='biases')
                self.visual_representations = tf.reshape(tf.nn.relu(
                    tf.nn.bias_add(tf.matmul(convolution_features, self.fully_connected_weight),
                                   self.fully_connected_bias)), shape=[self.batch_size, self.sample_size, 300])

            with tf.name_scope('lstm'):
                tf.logging.debug('Creating caption similarity model')
                self.lstm_cell = rnn.BasicLSTMCell(num_units=300, forget_bias=1.0, activation=tf.nn.relu)
                initial_state = self.lstm_cell.zero_state(self.batch_size, dtype=tf.float32)
                # Define weights
                self.lstm_out_weights = tf.Variable(tf.random_normal([300, 1]))
                self.lstm_out_bias = tf.Variable(tf.random_normal([1]))
                # This should be the similarities between RPN proposals and the given caption
                similarity = []
                for batch_id in range(self.batch_size):
                    q = self.q[batch_id]
                    batch_v = [self.visual_representations[batch_id]]
                    batch_similarity = []
                    for individual_visual_representation in tf.unstack(batch_v, axis=1):
                        v = tf.concat([individual_visual_representation, q], axis=0)
                        v = tf.reshape(v, shape=[1, -1, 300])
                        self.v = v
                        outputs, states = tf.nn.dynamic_rnn(self.lstm_cell, v, dtype=tf.float32)
                        self.outputs = outputs
                        last_output = tf.reshape(outputs[0][-1], shape=[1, -1])
                        result = tf.matmul(last_output, self.lstm_out_weights) + self.lstm_out_bias
                        self.result = result
                        batch_similarity.append(result)
                    similarity.append(tf.stack(batch_similarity))

                # Linear activation, using rnn inner loop last output
                self.similarities = tf.reshape(tf.stack(similarity), shape=[self.batch_size, self.sample_size])
                self.sigmoid_similarity = tf.nn.sigmoid(self.similarities)

                tf.summary.histogram('similarities', self.similarities)

            with tf.name_scope('loss'):
                tf.logging.debug('Creating loss, prediction and accuracy')
                diff = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.gt_similarity_batch,
                                                               logits=self.similarities)
                loss = tf.reduce_mean(diff)
                tf.summary.scalar('loss', loss)
                learning_rate = tf.train.exponential_decay(1E-3, self.global_step,
                                                           4096, 0.96, staircase=True)
                optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)
                self.train_step = optimizer.minimize(loss, global_step=self.global_step)

            with tf.name_scope('prediction'):
                correct_prediction = tf.cast(tf.equal(tf.argmax(self.similarities, axis=1),
                                                      tf.argmax(self.gt_similarity_batch, axis=1)), tf.float32)

            with tf.name_scope('accuracy'):
                self.accuracy = tf.reduce_mean(correct_prediction)
                tf.summary.scalar('accuracy', self.accuracy)

            self.global_initializer = tf.global_variables_initializer()
            self.local_initializer = tf.local_variables_initializer()
            self.summary_writer = tf.summary.FileWriter('summary/train_' + str(datetime.now()), self.inld_graph)
            self.merged_summaries = tf.summary.merge_all()
            self.saver = tf.train.Saver()
            tf.logging.debug('Graph Created.')

    def train(self):
        tf.logging.debug('Training Started')
        best_accuracy = 0.0
        with tf.Session(graph=self.inld_graph) as sess:
            if FLAGS.restore_inld_file != '':
                self.saver.restore(sess, FLAGS.restore_inld_file)
                tf.logging.info('Restored model from checkpoint' + FLAGS.restore_inld_file)
            else:
                sess.run(self.global_initializer)
                sess.run(self.local_initializer)
                self.vgg.load_weights('vgg16_weights.npz', sess)

            step = tf.train.global_step(sess, self.global_step)
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(coord=coord)
            while step <= FLAGS.max_iter:
                sess.run(self.train_step)
                step = tf.train.global_step(sess, self.global_step)
                if (step - 1) % 100 == 0:
                    acc, summary = sess.run([self.accuracy, self.merged_summaries])
                    if acc > best_accuracy:
                        best_accuracy = acc

                        save_path = self.saver.save(sess, "/scratch/data/mscoco2014/inld/checkpoint_300/model.ckpt",
                                                    global_step=step - 1)
                        tf.logging.info("New Best Accuracy Found. Model saved in file: %s" % save_path)
                    tf.logging.info('Training Iteration ' + str(step - 1) + ', Accuracy: ' + str(acc) + '.')
                    self.summary_writer.add_summary(summary, step - 1)
            coord.request_stop()
            coord.join(threads)

    def evaluate(self, model_checkpoint):
        tf.logging.debug('Evaluation Started')
        with tf.Session(graph=self.inld_graph) as sess:
            self.saver.restore(sess, model_checkpoint)
            tf.logging.info('Restored model from checkpoint' + model_checkpoint)
            total = 0.0
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(coord=coord)
            for i in range(100):
                acc = sess.run(self.accuracy)
                total += acc
                tf.logging.debug('Evaluation Iteration ' + str(i) + ', Accuracy: ' + str(acc) + '.')
            coord.request_stop()
            coord.join(threads)
            tf.logging.info('Evaluation Average Accuracy: ' + str(total / 100.0))

    def evaluate_with_output(self, model_checkpoint):
        tf.logging.debug('Evaluation Started')
        with tf.Session(graph=self.inld_graph) as sess:
            self.saver.restore(sess, model_checkpoint)
            tf.logging.info('Restored model from checkpoint' + model_checkpoint)
            coord = tf.train.Coordinator()
            threads = tf.train.start_queue_runners(coord=coord)
            [images, caption, similarity] = sess.run([self.images, self.caption_batch, self.sigmoid_similarity])
            import numpy as np
            import cv2
            images = np.reshape(images, newshape=(64, 2, 224, 224, 3))
            cv2.imwrite("image0.png", images[0, 0, :, :, :])
            cv2.imwrite("image1.png", images[0, 1, :, :, :])
            tf.logging.info((str(caption[0])))
            coord.request_stop()
            coord.join(threads)


if __name__ == '__main__':
    FLAGS(sys.argv)
    tf.logging.set_verbosity(tf.logging.DEBUG)
    ImageNaturalLanguageDetector(FLAGS.inld_training_file).train()
    ImageNaturalLanguageDetector(FLAGS.inld_validation_file).evaluate_with_output(
        '/scratch/data/mscoco2014/inld/checkpoint_300_relulstm_notfinetune/model.ckpt-39300')
